{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"12345678\"))\n",
    "\n",
    "def get_related_keywords(keyword):\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE toLower(n.name) CONTAINS toLower($keyword)\n",
    "    MATCH (n)--(related)\n",
    "    RETURN DISTINCT related.name AS keyword, labels(related) AS type\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, keyword=keyword)\n",
    "        return [record[\"keyword\"] for record in result]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pompe à chaleur', 'Plancher chauffant', 'Isolation thermique']\n"
     ]
    }
   ],
   "source": [
    "print(get_related_keywords(\"chauffage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"CHUNKS\\\\file_chunks.csv\") \n",
    "model = SentenceTransformer(\"dangvantuan/sentence-camembert-base\")\n",
    "\n",
    "# Let's quickly check the embedding dimension:\n",
    "dummy_vector = model.encode(\"texte de test\")\n",
    "dim = len(dummy_vector)  # This will be used in Qdrant's vector params\n",
    "print(f\"Embedding dimension: {dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_17252\\3964713409.py:6: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "11630it [03:17, 58.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Adjust host/port if Qdrant is elsewhere:\n",
    "client = QdrantClient(url=\"http://localhost:6333\")  # or QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "# Create or recreate a collection\n",
    "collection_name = \"PROJET_RCRA2\"\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# Now, embed and upsert each CSV chunk as a separate point in Qdrant\n",
    "points_to_upsert = []\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    text_chunk = row[\"chunk\"]\n",
    "    file_id = row[\"file\"]  # might be useful metadata\n",
    "    vector = model.encode(text_chunk)\n",
    "\n",
    "    # Create a unique ID for each chunk row\n",
    "    point_id = idx  \n",
    "    payload = {\n",
    "        \"file\": file_id, \n",
    "        \"chunk\": text_chunk\n",
    "    }\n",
    "\n",
    "    # Construct a PointStruct\n",
    "    points_to_upsert.append(\n",
    "        PointStruct(\n",
    "            id=point_id,\n",
    "            vector=vector.tolist(),\n",
    "            payload=payload\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded batch 0 to 100 from 11630\n",
      "✅ Uploaded batch 100 to 200 from 11630\n",
      "✅ Uploaded batch 200 to 300 from 11630\n",
      "✅ Uploaded batch 300 to 400 from 11630\n",
      "✅ Uploaded batch 400 to 500 from 11630\n",
      "✅ Uploaded batch 500 to 600 from 11630\n",
      "✅ Uploaded batch 600 to 700 from 11630\n",
      "✅ Uploaded batch 700 to 800 from 11630\n",
      "✅ Uploaded batch 800 to 900 from 11630\n",
      "✅ Uploaded batch 900 to 1000 from 11630\n",
      "✅ Uploaded batch 1000 to 1100 from 11630\n",
      "✅ Uploaded batch 1100 to 1200 from 11630\n",
      "✅ Uploaded batch 1200 to 1300 from 11630\n",
      "✅ Uploaded batch 1300 to 1400 from 11630\n",
      "✅ Uploaded batch 1400 to 1500 from 11630\n",
      "✅ Uploaded batch 1500 to 1600 from 11630\n",
      "✅ Uploaded batch 1600 to 1700 from 11630\n",
      "✅ Uploaded batch 1700 to 1800 from 11630\n",
      "✅ Uploaded batch 1800 to 1900 from 11630\n",
      "✅ Uploaded batch 1900 to 2000 from 11630\n",
      "✅ Uploaded batch 2000 to 2100 from 11630\n",
      "✅ Uploaded batch 2100 to 2200 from 11630\n",
      "✅ Uploaded batch 2200 to 2300 from 11630\n",
      "✅ Uploaded batch 2300 to 2400 from 11630\n",
      "✅ Uploaded batch 2400 to 2500 from 11630\n",
      "✅ Uploaded batch 2500 to 2600 from 11630\n",
      "✅ Uploaded batch 2600 to 2700 from 11630\n",
      "✅ Uploaded batch 2700 to 2800 from 11630\n",
      "✅ Uploaded batch 2800 to 2900 from 11630\n",
      "✅ Uploaded batch 2900 to 3000 from 11630\n",
      "✅ Uploaded batch 3000 to 3100 from 11630\n",
      "✅ Uploaded batch 3100 to 3200 from 11630\n",
      "✅ Uploaded batch 3200 to 3300 from 11630\n",
      "✅ Uploaded batch 3300 to 3400 from 11630\n",
      "✅ Uploaded batch 3400 to 3500 from 11630\n",
      "✅ Uploaded batch 3500 to 3600 from 11630\n",
      "✅ Uploaded batch 3600 to 3700 from 11630\n",
      "✅ Uploaded batch 3700 to 3800 from 11630\n",
      "✅ Uploaded batch 3800 to 3900 from 11630\n",
      "✅ Uploaded batch 3900 to 4000 from 11630\n",
      "✅ Uploaded batch 4000 to 4100 from 11630\n",
      "✅ Uploaded batch 4100 to 4200 from 11630\n",
      "✅ Uploaded batch 4200 to 4300 from 11630\n",
      "✅ Uploaded batch 4300 to 4400 from 11630\n",
      "✅ Uploaded batch 4400 to 4500 from 11630\n",
      "✅ Uploaded batch 4500 to 4600 from 11630\n",
      "✅ Uploaded batch 4600 to 4700 from 11630\n",
      "✅ Uploaded batch 4700 to 4800 from 11630\n",
      "✅ Uploaded batch 4800 to 4900 from 11630\n",
      "✅ Uploaded batch 4900 to 5000 from 11630\n",
      "✅ Uploaded batch 5000 to 5100 from 11630\n",
      "✅ Uploaded batch 5100 to 5200 from 11630\n",
      "✅ Uploaded batch 5200 to 5300 from 11630\n",
      "✅ Uploaded batch 5300 to 5400 from 11630\n",
      "✅ Uploaded batch 5400 to 5500 from 11630\n",
      "✅ Uploaded batch 5500 to 5600 from 11630\n",
      "✅ Uploaded batch 5600 to 5700 from 11630\n",
      "✅ Uploaded batch 5700 to 5800 from 11630\n",
      "✅ Uploaded batch 5800 to 5900 from 11630\n",
      "✅ Uploaded batch 5900 to 6000 from 11630\n",
      "✅ Uploaded batch 6000 to 6100 from 11630\n",
      "✅ Uploaded batch 6100 to 6200 from 11630\n",
      "✅ Uploaded batch 6200 to 6300 from 11630\n",
      "✅ Uploaded batch 6300 to 6400 from 11630\n",
      "✅ Uploaded batch 6400 to 6500 from 11630\n",
      "✅ Uploaded batch 6500 to 6600 from 11630\n",
      "✅ Uploaded batch 6600 to 6700 from 11630\n",
      "✅ Uploaded batch 6700 to 6800 from 11630\n",
      "✅ Uploaded batch 6800 to 6900 from 11630\n",
      "✅ Uploaded batch 6900 to 7000 from 11630\n",
      "✅ Uploaded batch 7000 to 7100 from 11630\n",
      "✅ Uploaded batch 7100 to 7200 from 11630\n",
      "✅ Uploaded batch 7200 to 7300 from 11630\n",
      "✅ Uploaded batch 7300 to 7400 from 11630\n",
      "✅ Uploaded batch 7400 to 7500 from 11630\n",
      "✅ Uploaded batch 7500 to 7600 from 11630\n",
      "✅ Uploaded batch 7600 to 7700 from 11630\n",
      "✅ Uploaded batch 7700 to 7800 from 11630\n",
      "✅ Uploaded batch 7800 to 7900 from 11630\n",
      "✅ Uploaded batch 7900 to 8000 from 11630\n",
      "✅ Uploaded batch 8000 to 8100 from 11630\n",
      "✅ Uploaded batch 8100 to 8200 from 11630\n",
      "✅ Uploaded batch 8200 to 8300 from 11630\n",
      "✅ Uploaded batch 8300 to 8400 from 11630\n",
      "✅ Uploaded batch 8400 to 8500 from 11630\n",
      "✅ Uploaded batch 8500 to 8600 from 11630\n",
      "✅ Uploaded batch 8600 to 8700 from 11630\n",
      "✅ Uploaded batch 8700 to 8800 from 11630\n",
      "✅ Uploaded batch 8800 to 8900 from 11630\n",
      "✅ Uploaded batch 8900 to 9000 from 11630\n",
      "✅ Uploaded batch 9000 to 9100 from 11630\n",
      "✅ Uploaded batch 9100 to 9200 from 11630\n",
      "✅ Uploaded batch 9200 to 9300 from 11630\n",
      "✅ Uploaded batch 9300 to 9400 from 11630\n",
      "✅ Uploaded batch 9400 to 9500 from 11630\n",
      "✅ Uploaded batch 9500 to 9600 from 11630\n",
      "✅ Uploaded batch 9600 to 9700 from 11630\n",
      "✅ Uploaded batch 9700 to 9800 from 11630\n",
      "✅ Uploaded batch 9800 to 9900 from 11630\n",
      "✅ Uploaded batch 9900 to 10000 from 11630\n",
      "✅ Uploaded batch 10000 to 10100 from 11630\n",
      "✅ Uploaded batch 10100 to 10200 from 11630\n",
      "✅ Uploaded batch 10200 to 10300 from 11630\n",
      "✅ Uploaded batch 10300 to 10400 from 11630\n",
      "✅ Uploaded batch 10400 to 10500 from 11630\n",
      "✅ Uploaded batch 10500 to 10600 from 11630\n",
      "✅ Uploaded batch 10600 to 10700 from 11630\n",
      "✅ Uploaded batch 10700 to 10800 from 11630\n",
      "✅ Uploaded batch 10800 to 10900 from 11630\n",
      "✅ Uploaded batch 10900 to 11000 from 11630\n",
      "✅ Uploaded batch 11000 to 11100 from 11630\n",
      "✅ Uploaded batch 11100 to 11200 from 11630\n",
      "✅ Uploaded batch 11200 to 11300 from 11630\n",
      "✅ Uploaded batch 11300 to 11400 from 11630\n",
      "✅ Uploaded batch 11400 to 11500 from 11630\n",
      "✅ Uploaded batch 11500 to 11600 from 11630\n",
      "✅ Uploaded batch 11600 to 11630 from 11630\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100  # you can tweak this\n",
    "\n",
    "for i in range(0, len(points_to_upsert), BATCH_SIZE):\n",
    "    batch = points_to_upsert[i:i + BATCH_SIZE]\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=batch\n",
    "    )\n",
    "    print(f\"✅ Uploaded batch {i} to {i + len(batch)} from {len(points_to_upsert)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_neo4j(user_query):\n",
    "    \"\"\"\n",
    "    Naive approach: split user query into tokens \n",
    "    and gather related keywords from Neo4j.\n",
    "    \"\"\"\n",
    "    tokens = user_query.lower().split()\n",
    "    all_expanded = set()\n",
    "    for token in tokens:\n",
    "        expansions = get_related_keywords(token)\n",
    "        all_expanded.update(expansions)\n",
    "    return list(all_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(user_query, expansions, top_k=5):\n",
    "    \"\"\"\n",
    "    - Combine the user query with expansions\n",
    "    - Embed once\n",
    "    - Query Qdrant for the top_k most similar chunks\n",
    "    \"\"\"\n",
    "    if expansions:\n",
    "        expansion_text = \" \".join(expansions)\n",
    "        augmented_text = user_query + \" \" + expansion_text\n",
    "    else:\n",
    "        augmented_text = user_query\n",
    "    \n",
    "    query_vector = model.encode(augmented_text)\n",
    "\n",
    "    # Qdrant query\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    # 'results' will be a list of ScoredPoint objects\n",
    "    # Each contains 'payload' (with our stored chunk)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(user_query, top_k=5):\n",
    "    # 1) Expand user query with Neo4j\n",
    "    expansions = expand_query_with_neo4j(user_query)\n",
    "    # 2) Retrieve relevant chunks from Qdrant\n",
    "    docs = retrieve_relevant_chunks(user_query, expansions, top_k=top_k)\n",
    "    context_text = \"\"\n",
    "    for doc in docs:\n",
    "        payload = doc[1][0].payload\n",
    "        context_text += f\"Fichier: {payload['file']}\\nChunk: {payload['chunk']}\\n\\n\"\n",
    "    full_prompt = f\"\"\"\n",
    "    CONTEXTE:\n",
    "    {context_text}\n",
    "\n",
    "    QUESTION UTILISATEUR:\n",
    "    {user_query}\n",
    "\n",
    "    RÉPONSE ATTENDUE (en utilisant uniquement les informations ci-dessus) :\n",
    "    \"\"\"\n",
    "    return full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ventilation', 'Chauffage', 'Isolation thermique', \"Membrane d'étanchéité\", 'Chauffe-eau thermodynamique', 'RE2020', 'VMC double flux', 'Pompe à chaleur', 'Plancher chauffant', 'Énergie renouvelable']\n"
     ]
    }
   ],
   "source": [
    "query = \"Quels sont les avantages d’utiliser une pompe à chaleur avec un plancher chauffant ?\"\n",
    "query = expand_query(query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(prompt, model=\"llama3.2\"):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False  # you can use True if you want streamed responses\n",
    "        }\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data[\"response\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les avantages de l'utilisation d'une pompe à chaleur avec un plancher chauffant sont :\n",
      "\n",
      "1. Chauffage thermodynamique basse température : Cette technologie permet un chauffage efficace et respectueux de l'environnement, en utilisant les énergies renouvelables pour chauffer le plancher.\n",
      "2. Suppression du bouclage ECS (Équilibreur de Charge Solaire) : En utilisant une pompe à chaleur avec un plancher chauffant, il n'est pas nécessaire d'installer un système de bouclage ECS, ce qui réduit les coûts et l'importance des déchets thermiques.\n",
      "3. Économie d'énergie : Le chauffage basse température peut réduire la consommation d'énergie pour chauffer le plancher, ce qui peut entraîner une réduction des factures de gaz ou d'électricité.\n",
      "4. Sécurité et confort : L'utilisation d'une pompe à chaleur avec un plancher chauffant peut améliorer la sécurité et le confort dans les bâtiments, en particulier pour les personnes âgées ou handicapées.\n",
      "5. Double ou triple service : Cette technologie permet de fournir plusieurs services (chauffage, éclairage et climatisation) à partir d'un même système, ce qui peut réduire la complexité du système et améliorer l'efficacité énergétique.\n",
      "\n",
      "Il est important de noter que ces avantages sont étayés par les informations fournies dans le fichier \"La Pompe à chaleur, des solutions disponibles en habitat collectif.md\".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = query_ollama(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
